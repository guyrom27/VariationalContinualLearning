{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Uniform\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "class mlp_layer(nn.Module):\n",
    "    def __init__(self,d_in, d_out, activation):\n",
    "        \"\"\"\n",
    "        Activation is a function (eg. torch.nn.functional.sigmoid/relu)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mu = nn.Linear(d_in, d_out)\n",
    "        self._init_weights(d_in, d_out)\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.activation(self.mu(x))\n",
    "\n",
    "    def _init_weights(self, input_size, output_size, constant=1.0):\n",
    "        scale = constant*np.sqrt(6.0/(input_size + output_size))\n",
    "        assert(output_size > 0)\n",
    "        nn.init.uniform_(self.mu.weight, -scale, scale)\n",
    "        nn.init.zeros_(self.mu.bias)\n",
    "        \n",
    "    @property\n",
    "    def d_out(self):\n",
    "        return self.mu.weight.shape[0]\n",
    "    \n",
    "    @property\n",
    "    def d_in(self):\n",
    "        return self.mu.weight.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal\n",
    "import copy\n",
    "\n",
    "\n",
    "class bayesian_mlp_layer(mlp_layer):\n",
    "    def __init__(self,d_in, d_out, activation):\n",
    "        \"\"\"\n",
    "        Activation is a function (eg. torch.nn.functional.sigmoid/relu)\n",
    "        \"\"\"\n",
    "        super().__init__(d_in,d_out,activation)\n",
    "        self.log_sigma = nn.Linear(d_in, d_out)\n",
    "        self._init_log_sigma()\n",
    "        #mu is initialized the same as non-Bayesian mlp\n",
    "        \n",
    "        self.weight_sampler = Normal(self.mu.weight, \\\n",
    "                              torch.exp(self.log_sigma.weight))\n",
    "        self.bias_sampler = Normal(self.mu.bias, \\\n",
    "                              torch.exp(self.log_sigma.bias))\n",
    "        \n",
    "    def forward(self,x, sampling=True):\n",
    "        if sampling:\n",
    "            my_lin = nn.Linear(*self.mu.weight.shape)\n",
    "            my_lin.weight = nn.Parameter(self.weight_sampler.sample())\n",
    "            my_lin.bias = nn.Parameter(self.bias_sampler.sample())\n",
    "            return self.activation(my_lin(x))\n",
    "        else:\n",
    "            return super().forward(x)    \n",
    "        \n",
    "    def _init_log_sigma(self):\n",
    "        nn.init.constant_(self.log_sigma.weight, 0.0)\n",
    "        nn.init.constant_(self.log_sigma.bias, 0.0)\n",
    "    \n",
    "    def get_posterior(self):\n",
    "        return [(self.mu.weight, self.log_sigma.weight),(self.mu.bias, self.log_sigma.bias)]\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalSamplingLayer:\n",
    "    def __init__(self,d_out):\n",
    "        self.d_out = d_out\n",
    "    \n",
    "    def __call__(self, mu_log_sigma_vec):\n",
    "        return Normal(mu_log_sigma_vec[:,:self.d_out], torch.exp(mu_log_sigma_vec[:,self.d_out:])).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "class FunctionComposition:\n",
    "    def __init__(self,f_list):\n",
    "        assert(len(f_list) > 0)\n",
    "        for i in range(len(f_list)-1):\n",
    "            assert(f_list[i].d_out == f_list[i+1].d_in)\n",
    "        self.f_list = f_list\n",
    "    \n",
    "    def __call__(self, x, *optional):\n",
    "        for f in self.f_list:\n",
    "            x = f(x,*optional)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return list(itertools.chain(*list(map(lambda f: f.parameters(), self.f_list))))\n",
    "    \n",
    "    @property\n",
    "    def d_in(self):\n",
    "        return self.f_list[0].d_in\n",
    "    \n",
    "    @property\n",
    "    def d_out(self):\n",
    "        return self.f_list[-1].d_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "class BayesianNet(FunctionComposition):\n",
    "    def __init__(self,f_list):\n",
    "        super().__init__(f_list)\n",
    "    \n",
    "    def get_posterior(self):\n",
    "        return list(itertools.chain(*list(map(lambda f: f.get_posterior(), self.f_list))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNFactory:\n",
    "    @classmethod\n",
    "    def CreateNN(cls, dims, activations):\n",
    "        assert(len(dims)-1 == len(activations))\n",
    "        layers = []\n",
    "        for i in range(len(dims)-1):\n",
    "            layers.append(mlp_layer(dims[i],dims[i+1],activations[i]))\n",
    "        return FunctionComposition(layers)\n",
    "    \n",
    "    @classmethod\n",
    "    def CreateBayesianNet(cls, dims, activations):\n",
    "        assert(len(dims)-1 == len(activations))\n",
    "        layers = []\n",
    "        for i in range(len(dims)-1):\n",
    "            layers.append(bayesian_mlp_layer(dims[i],dims[i+1],activations[i]))\n",
    "        return BayesianNet(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_div_gaussian(mu_q, log_sig_q, mu_p, log_sig_p): \n",
    "    \"\"\"\n",
    "    KL(q||p), gets log of sigma rather than sigma\n",
    "    \"\"\"\n",
    "    return log_sig_p - log_sig_q + (0.5)*torch.exp(-2*log_sig_p)*(torch.exp(log_sig_q)**2 + (mu_q - mu_p)**2)-1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_div_gaussian_from_standard_normal(mu_q, log_sig_q):\n",
    "    #0,0 corresponds to N(0,1) due to the log_sig representation, works for multidim normal as well.\n",
    "    return KL_div_gaussian(mu_q, log_sig_q, torch.zeros(1), torch.zeros(1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Zs_to_mu_sig(Zs_params):\n",
    "    dimZ = Zs_params.shape[1]//2 #1st is batch size 2nd is 2*dimZ\n",
    "    mu_qz = Zs_params[:,:dimZ]\n",
    "    log_sig_qz  = Zs_params[:,dimZ:]\n",
    "    return mu_qz, log_sig_qz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forced_interval = (1e-9, 1.0)\n",
    "def log_bernouillli(Xs, Mu_Reconstructed_Xs):\n",
    "    \"\"\"\n",
    "    Mu_Reconstructed_X is the output of the decoder. We accept fractions, and project them to the interval 'forced_interval' for numerical stability\n",
    "    \"\"\"\n",
    "    logprob =    Xs      * torch.log(torch.clamp(Mu_Reconstructed_Xs,         *forced_interval)) \\\n",
    "              + (1 - Xs) * torch.log(torch.clamp((1.0 - Mu_Reconstructed_Xs), *forced_interval))\n",
    "    return torch.sum(logprob, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_P_y_GIVEN_x(Xs, enc, sample_and_decode, NumLogPSamples = 100):\n",
    "    \"\"\"\n",
    "    Returns logP(Y|X), KL(Z||Normal(0,1))\n",
    "    \"\"\"\n",
    "    Zs_params = enc(Xs)\n",
    "    mu_qz, log_sig_qz  = Zs_to_mu_sig(Zs_params)\n",
    "    kl_z = KL_div_gaussian_from_standard_normal(mu_qz, log_sig_qz)\n",
    "    logp = 0.0\n",
    "    for _ in range(NumLogPSamples):\n",
    "        #The Zs_params are the deterministic result of enc(Xs) so we don't recalculate them\n",
    "        Mu_Ys = sample_and_decode(Zs_params)\n",
    "        logp += log_bernouillli(Xs, Mu_Ys) / NumLogPSamples\n",
    "    return logp, kl_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedDecoder(nn.Module):\n",
    "    def __init__(self, dims, activations):\n",
    "        super().__init__()\n",
    "        self.net = NNFactory.CreateBayesianNet(dims, activations)\n",
    "        self._init_prior()\n",
    "        \n",
    "    def __call__(self, Xs):\n",
    "        return self.net(Xs)\n",
    "    \n",
    "    def _init_prior(self):\n",
    "        \"\"\"\n",
    "        Initialize a constant tensor that corresponds to a prior distribution over all the weights\n",
    "        which is standard normal\n",
    "        \"\"\"\n",
    "        self.prior = [(torch.zeros(mu.shape),torch.zeros(log_sig.shape)) for mu,log_sig in self.net.get_posterior()]\n",
    "        \n",
    "    def update_prior(self):\n",
    "        \"\"\"\n",
    "        Copy the current posterior to a constant tensor, which will be used as prior for the next task\n",
    "        \"\"\"\n",
    "        self.prior = [(mu.clone().detach(), log_sig.clone().detach()) for mu,log_sig  in self.net.get_posterior()]\n",
    "        \n",
    "    def KL_from_prior(self):\n",
    "        KLs=[KL_div_gaussian(*post, *prior) for (post, prior) in zip(self.net.get_posterior(), self.prior)]\n",
    "        return sum(map(torch.sum, KLs))\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.net.parameters()\n",
    "    \n",
    "    @property\n",
    "    def d_in(self):\n",
    "        return self.net.d_in\n",
    "    \n",
    "    @property\n",
    "    def d_out(self):\n",
    "        return self.net.d_out\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim \n",
    "\n",
    "#BayesianVAE\n",
    "class TaskModel(nn.Module):\n",
    "    def __init__(self, enc_dims_activations, dec_head_dims_activations, dec_shared, learning_rate = 1e-4):\n",
    "        super().__init__()\n",
    "        self.enc = NNFactory.CreateNN(*enc_dims_activations)\n",
    "        self.dec_head = NNFactory.CreateBayesianNet(*dec_head_dims_activations)\n",
    "        self.dec_shared = dec_shared\n",
    "    \n",
    "        self.sampler = NormalSamplingLayer(self.dec_head.d_in)\n",
    "        self.sample_and_decode = FunctionComposition([self.sampler, self.dec_head, self.dec_shared])\n",
    " \n",
    "        #update just before training\n",
    "        self.DatasetSize = None\n",
    "        \n",
    "        #Guards from retraining- train only once\n",
    "        self.TrainGuard = True\n",
    "        \n",
    "        self.optimizer = self._create_optimizer(learning_rate)\n",
    "    \n",
    "    def forward(self, Xs):\n",
    "        logp, kl_z = log_P_y_GIVEN_x(Xs, self.enc, self.sample_and_decode)\n",
    "        kl_shared_dec_Qt_2_PREV_Qt = self.dec_shared.KL_from_prior()\n",
    "        #We ignore the kl(private dec || Normal(0,1) ) like the authors did\n",
    "        ELBO = torch.mean(logp) - torch.mean(kl_z) - (kl_shared_dec_Qt_2_PREV_Qt / self.DatasetSize)\n",
    "        return -ELBO\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.enc.parameters()  + self.dec_shared.parameters() + self.dec_head.parameters()  \n",
    "        \n",
    "    def _create_optimizer(self, learning_rate):\n",
    "        return torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def _update_prior(self):\n",
    "        self.dec_shared.update_prior()\n",
    "        #no other priors should be updated. they are trained once.\n",
    "        return\n",
    "    \n",
    "    def train(self, n_epochs, task_trainloader):\n",
    "        #We don't intend a TaskModel to be trained more than once\n",
    "        assert(self.TrainGuard)\n",
    "        self.TrainGuard = False\n",
    "        \n",
    "        self.DatasetSize = len(task_trainloader.dataset)\n",
    "        #This will set the prior to the current posterior, before we start to change it during training\n",
    "        self._update_prior()\n",
    "        # loop over the dataset multiple times\n",
    "        for epoch in range(n_epochs):  \n",
    "            print(\"starting epoch \" + str(epoch))\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(task_trainloader):\n",
    "                print(\"next batch\")\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "        \n",
    "                # step\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self(inputs.view(-1,self.enc.d_in))\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "        \n",
    "                # print statistics\n",
    "                running_loss += loss.item() #?\n",
    "                if True: #if i % 10 == 1999:    # print every 2000 mini-batches\n",
    "                    print('[%d, %5d] loss: %.3f' %\n",
    "                          (epoch + 1, i + 1, running_loss / 2000))\n",
    "                    running_loss = 0.0\n",
    "        self.DatasetSize = None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "import torchvision\n",
    "\n",
    "def create_mnist_single_digit_loaders(b_size=10, train_data=True):\n",
    "    \n",
    "    dataset = torchvision.datasets.MNIST(root='./data', train=train_data, download=True, transform=torchvision.transforms.ToTensor())\n",
    "    \n",
    "    for i in range(10):\n",
    "        partial_dataset = torch.utils.data.Subset(dataset, torch.nonzero(dataset.train_labels==i).squeeze()) \n",
    "        \n",
    "        #NOT Repeating the original \"mistake\"\n",
    "        #train_idx = len(partial_trainset) * 0.9         \n",
    "        partial_loader = torch.utils.data.DataLoader(partial_dataset, batch_size=b_size,shuffle=True)\n",
    "        yield partial_loader\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimX=28*28\n",
    "dimH=500\n",
    "dimZ=50\n",
    "batch_size = 50\n",
    "n_epochs = 1 #200\n",
    "\n",
    "#Shared decoder\n",
    "dec_shared_dims=[dimH, dimH, dimX]\n",
    "dec_shared_activations=[F.relu,torch.sigmoid]\n",
    "\n",
    "#Encoder\n",
    "enc_dims=[dimX, dimH, dimH, dimZ*2]\n",
    "enc_activations=[F.relu,F.relu,lambda x:x]\n",
    "\n",
    "#Private decoder (Head)\n",
    "dec_head_dims=[dimZ, dimH, dimH]\n",
    "dec_head_activations=[F.relu,F.relu]\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    dec_shared = SharedDecoder(dec_shared_dims, dec_shared_activations) \n",
    "    \n",
    "    #TODO: check dec_shared.parameters()\n",
    "    \n",
    "    #this can be any iterable\n",
    "    task_loaders = list(create_mnist_single_digit_loaders(batch_size))\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    #this may train the classifier to generate test_classifier\n",
    "    #evaluator = Evaluations()\n",
    "    \n",
    "    #A task corresponds to a digit\n",
    "    for task_id, loader in enumerate(task_loaders):\n",
    "        print(\"starting task \" +str(task_id))\n",
    "        task_model = TaskModel((enc_dims, enc_activations), \\\n",
    "                               (dec_head_dims,dec_head_activations), \\\n",
    "                               dec_shared)\n",
    "        models.append(task_model)\n",
    "        print(\"starting training\")\n",
    "        task_model.train(n_epochs, loader)\n",
    "        #make sure you don't change the model params inside the eval \n",
    "        #evaluator.create_task_evaluations(models)\n",
    "    \n",
    "    #evaluator.report_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(iter(create_mnist_single_digit_loaders()))\n",
    "print((next(iter(a)))[0].shape)\n",
    "batch1 = (next(iter(a)))[0]\n",
    "\n",
    "import torch.nn\n",
    "l = nn.Linear(28*28, 17)\n",
    "b = batch1.view((10,784))\n",
    "l(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(batch1.view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(torch.ones(50,784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
